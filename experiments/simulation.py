# !/usr/bin/env python
# -*- coding:utf-8 -*-
# Author:
# The implementation of the simulation experiments

import argparse
import yaml
from pathlib import Path
from utils import read_data, parse_path
from min_density import MinD, preprocess_txs
import numpy as np
import copy
import time


def split_batches(transactions, subsets, exp_block_count):
    # compute batch capacity based on parameters
    sum_gas = 0
    for tx in transactions:
        sum_gas += tx[2]
    capacity = int(sum_gas / exp_block_count)
    # print(capacity)

    # filter out the transactions that cannot fit in one batch
    # to prevent infinity loop
    i = 0
    while i < len(transactions):
        tx = transactions[i]
        if tx[2] > capacity:
            transactions.remove(tx)
        else:
            i += 1

    # continue picking up batches from the candidate transactions
    # by using MinD algorithm based on batch capacity
    batches = []
    access_counter = [0 for i in range(subsets)]

    while (len(transactions) > 0):
        # based on the prewrite condition, select transactions that avoid the dependency
        if sum(access_counter) == access_counter[0] * subsets:
            candidates = transactions
            accessable = subsets
        else:
            accessable_sets = []
            for ind in range(len(access_counter)):
                if access_counter[ind] < max(access_counter):
                    accessable_sets.append(ind)
            candidates, cap = [], 0
            for tx in transactions:
                if set(tx[1]).intersection(accessable_sets) == set(tx[1]):
                    candidates.append(tx)
                    cap += tx[2]
            # if candidates cannot fill in one batch, we allow all transactions to be selectable
            if cap < capacity:
                candidates = transactions
                accessable = subsets
            else:
                accessable = len(accessable_sets)

        grouped_transactions = preprocess_txs(candidates)
        (opt_txs, _) = MinD(accessable, capacity, candidates, grouped_transactions)

        cv = set()
        i = 0
        while i < len(transactions):
            tx = transactions[i]
            if tx[0] in opt_txs:
                cv |= set(tx[1])
                transactions.remove(tx)
            else:
                i += 1

        # maintain the access counter to each set
        for st in cv:
            access_counter[st] += 1

        batches.append((opt_txs, cv))   # id of transaction, covered subsets
    return batches


# pre-determine the information of each batch/block including the time to be on-chain etc.
def batch_information(batches, f_rate, sys_info):
    timer = 0
    batches_with_info = []

    for i in range(len(batches)):
        timer += round(np.random.normal(sys_info[0][0], sys_info[0][1]))
        on_chain_time = timer

        # simulate if the block is generated by a failure node
        # if so, the proof time is set to be 999999
        if i ==0:
            proof_time = timer + round(np.random.normal(sys_info[1][0], sys_info[1][1]))
        else:
            if np.random.random() >= f_rate:
                proof_time = timer + round(np.random.normal(sys_info[1][0], sys_info[1][1]))
            else:
                proof_time = 999999

        batches_with_info.append((batches[i], on_chain_time, proof_time))

    return batches_with_info


def simulate_serialized(batches, storage, tx_count):
    # the batch is in the format of:
    # ((TX IDs, covered subset index), prewrite on-chain time, proof get validation time)
    stages = [-1 for _ in range(len(batches))]   # 0: prewrite, 1: got valid proof, 2: committed
    commit_time = [0 for _ in range(len(batches))]

    on_chain_time, proof_time = [], []
    for i in range(len(batches)):
        on_chain_time.append(batches[i][1])
        proof_time.append(batches[i][2])

    on_chain_time, proof_time = np.array(on_chain_time), np.array(proof_time)

    counter = 0
    for timer in range(1, max(proof_time[proof_time < 999999]) + 1):
        # 1: make block on-chain
        if counter < len(batches):
            if on_chain_time[counter] == timer:
                stages[counter] = 0
                counter += 1

        # 2: check if obtained valid proof
        temp = np.where(proof_time==timer)[0].tolist()
        if len(temp) != 0:
            # 3: change the stage of each block and record the commit time
            for i in temp:
                if stages[i] == 0:
                    stages[i] = 1
                else:
                    raise RuntimeError
            for i in range(len(stages)):
                if stages[i] == 1:
                    stages[i] = 2
                    commit_time[i] = timer
                elif stages[i] == 2:
                    continue
                else:   # stage == 0, the block is only prewrite
                    break
        # 4: calculate the storage (ignored for serialized mode)

    # calculate the final result
    avg_latency = 0

    committed_block_count = 0
    for i in range(len(batches)):
        if commit_time[i] > 0:
            committed_block_count += 1
            avg_latency += (commit_time[i] - on_chain_time[i])

    duration = max(proof_time[proof_time < 999999]) - min(proof_time)

    # the multiplier is used to simulate and control how many zkSync prewrite/proof transactions are
    # packed into one ETH block. Since we use the block interval of ETH, meanwhile, each ETH block can
    # contain multiple zkSync TXNs, we use the multiplier to reflect the real TPS.
    # For more details, please refer to https://zksync.io/faq/tech.html#zk-rollup-architecture
    multiplier = 50
    avg_block_cap = multiplier * tx_count / len(batches)

    # avg_storage represent in the format of # of tree nodes and table entries
    return np.array([avg_latency/committed_block_count, committed_block_count * avg_block_cap / duration, storage], dtype=np.float32)


def simulate_CAT(batches, subsets, expire, storage):
    # the batch is in the format of:
    # ((TX IDs, covered subset index), prewrite on-chain time, proof get validation time)
    # stages 0: prewrite, 1: got valid proof, 2: committed, -1: aborted
    commit_time = [0 for _ in range(len(batches))]
    processing_table = [[''] * subsets for _ in range(len(batches))]

    on_chain_time, proof_time, expire_time = [], [], []
    for i in range(len(batches)):
        on_chain_time.append(batches[i][1])
        expire_time.append(batches[i][1] + expire)
        proof_time.append(batches[i][2])

    on_chain_time, proof_time, expire_time = np.array(on_chain_time), np.array(proof_time), np.array(expire_time)

    counter, avg_storage = 0, 0
    for timer in range(1, max(proof_time[proof_time < 999999]) + 1):
        # 1: make block on-chain
        if counter < len(batches):
            if on_chain_time[counter] == timer:
                _, cov = batches[counter][0]
                for i in cov:
                    processing_table[counter][i] = 0  # 0: prewrite stage
                counter += 1

        # 2ï¼šcheck expiration
        temp = np.where(expire_time == timer)[0].tolist()
        if len(temp) != 0:
            for i in temp:
                # if the block is still in prewrite, then abort it
                if 0 in processing_table[i]:
                    for j in range(len(processing_table[i])):
                        if processing_table[i][j] != '':
                            processing_table[i][j] = -1      # -1 : aborted (due to expire)

        # 3: check if obtained valid proof
        temp = np.where(proof_time == timer)[0].tolist()
        if len(temp) != 0:
            # 4: change the stage of each block and record the commit time
            # pre-commit phase
            for i in temp:
                for j in range(len(processing_table[i])):
                    if processing_table[i][j] == 0:
                        processing_table[i][j] = 1

            # commit phase: here we use a temp table to record the commit info
            # which is a simplified implementation of the CM-forest commit
            temp_table = copy.deepcopy(processing_table)
            for j in range(subsets):
                for i in range(len(batches)):
                    if temp_table[i][j] == '':   # skip the empty field (considered as the pointer to the next node)
                        continue
                    elif temp_table[i][j] == 1:
                        temp_table[i][j] = 2
                    elif temp_table[i][j] == 2:
                        continue
                    else:  # stage == 0 or -1, the block is only prewrite or aborted
                        break

            # compare temp table and original processing table to decide commit info
            for i in range(len(batches)):
                if 2 in temp_table[i] and 2 not in processing_table[i] and set(temp_table[i]).union({2, ''}).issubset({2, ''}):
                    processing_table[i] = copy.deepcopy(temp_table[i])
                    commit_time[i] = timer

        # 5: calculate the storage by enumerating each tree root
        # For stage 0, the node must be recorded
        # For stage 1, consecutive 1s will be merged (only 1 node is needed)
        # For stage 2, the node is merged and only the root will be recorded
        # For stage -1, the node and all nodes after it will be ignored
        if (timer >= min(on_chain_time) and timer <= max(on_chain_time)) or (timer >= min(proof_time) and timer <= max(proof_time[proof_time < 999999])):
            temp_storage = 0
            for j in range(subsets):
                stage_one, stage_two = False, False
                for i in range(len(batches)):
                    if processing_table[i][j] == '':
                        continue
                    elif processing_table[i][j] == -1:
                        break
                    elif processing_table[i][j] == 2:
                        if not stage_two:
                            temp_storage += 1
                            stage_two = True
                        else:
                            continue
                    elif processing_table[i][j] == 1:
                        if not stage_one:
                            temp_storage += 1
                            stage_one = True
                        else:
                            continue
                    else:   # stage = 0, we flip the condition of if start to count consecutive 1s
                        stage_one = False
                        temp_storage += 1

            avg_storage += temp_storage

    # calculate the final result
    avg_latency = 0
    committed_block_count = 0
    for i in range(len(batches)):
        if commit_time[i] > 0:
            committed_block_count += 1
            avg_latency += (commit_time[i] - on_chain_time[i])

    duration = max(proof_time[proof_time < 999999]) - min(proof_time)

    # same multiplier as the serialized simulation
    multiplier = 50
    avg_block_cap = multiplier * tx_count / len(batches)

    # for each entry, the storage includes: one CM, one stage, four addresses (2 for the CM forest, 2 for the entry table)
    storage_per_entry = storage[0] + storage[2] + 4 * storage[1]
    storage_counting_duration = max(on_chain_time) - min(on_chain_time) + duration + 2  # +2: both durations we have = coondition on the edges
    avg_storage = storage_per_entry * avg_storage / storage_counting_duration

    # avg_storage represent in the format of # of tree nodes and table entries
    return np.array([avg_latency / committed_block_count, committed_block_count * avg_block_cap / duration, avg_storage],
                    dtype=np.float32)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--cfg', type=str, default="../cfgs/simulate.yaml",
                        help='the configuration yaml file (see cfgs/simulate.yaml)')
    parser.add_argument('--name', type=str, default="simulate",
                        help='the name of the experiment')
    opt = parser.parse_args()

    if Path(opt.cfg).exists():
        with open(opt.cfg, encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        simulation_round = cfg['round']
        tx_count = cfg['tx_count']  # 512
        exp_block_count = cfg['exp_block_count']   # 30 by default
        seed_list = cfg['seed_list']
        f_rates = cfg['failure_rates']
        size_dist = cfg['size_dist']
        exp_type = cfg['exp_type']

        if exp_type == 'real_data':
            subsets_list = cfg['subsets_list']  # 512 by default
            zipfian_list = [0]
        else:
            subsets_list = [cfg['default_subsets']]
            zipfian_list = cfg['zipfian_list']

        # load parameters for batch information
        on_chain = (cfg['avg_on_chain_time'], cfg['on_chain_time_std'])
        proof_time_dict, proof_time_std_dict = cfg['avg_proof_time_dict'], cfg['proof_time_std_dict']
        sys_size = (cfg['commitment_size'], cfg['addr_size'], cfg['stage_size'])

        overall_states = cfg['overall_states']
        original_sys_info = (on_chain,
                             (proof_time_dict[overall_states], proof_time_dict[overall_states] * proof_time_std_dict[overall_states]),
                             sys_size)

        # varying four parameters
        for subsets in subsets_list:
            # configure the system
            ind = overall_states - int(np.log2(subsets))
            # sys_info: (on_chain time, proof time, storage)
            sys_info = (on_chain,
                        (proof_time_dict[ind], proof_time_dict[ind] * proof_time_std_dict[ind]),
                        sys_size)
            tau = round((3 * sys_info[1][1] + 1) * sys_info[1][0] / sys_info[0][0])
            expire_time = round(tau * sys_info[0][0])

            for zipf in zipfian_list:
                for rate in f_rates:  # < rate: fail, >= rate: success
                    print("processing the case with {} subsets".format(subsets))
                    start = time.time()
                    # simulation results for serialized and CAT
                    s_results, c_results = np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0])

                    for seed in seed_list:
                        if exp_type == 'real_data':
                            transactions = read_data(parse_path(tx_count, subsets, seed))
                        else:
                            transactions = read_data(parse_path(tx_count, subsets, seed, size_distribution = size_dist, zipfian=zipf))

                        batches = split_batches(transactions, subsets, exp_block_count)

                        for _ in range(simulation_round):  # total round number = round * seeds (for each transaction file)
                            s_bat_with_info = batch_information(batches, rate, original_sys_info)
                            c_bat_with_info = batch_information(batches, rate, sys_info)

                            s_results += simulate_serialized(s_bat_with_info, sys_info[2][0], tx_count)
                            c_results += simulate_CAT(c_bat_with_info, subsets, expire_time, sys_info[2])

                    s_results = s_results / simulation_round / len(seed_list)
                    c_results = c_results / simulation_round / len(seed_list)

                    if exp_type == 'real_data':
                        root_path = Path("../outputs/simulation/")
                        root_path.mkdir(parents=True, exist_ok=True)
                        s_path = root_path / "serialized_{}_round_{}_TXs_{}_subsets_{}_exp_blocks.txt".format(
                            simulation_round, tx_count, subsets, exp_block_count)
                        c_path = root_path / "cat_{}_round_{}_TXs_{}_subsets_{}_exp_blocks.txt".format(
                            simulation_round, tx_count, subsets, exp_block_count)
                    else:
                        root_path = Path("../outputs/simulation_syn/")
                        root_path.mkdir(parents=True, exist_ok=True)
                        s_path = root_path / "serialized_{}_round_{}_TXs_{}_subsets_{}_exp_blocks_{}_zipf.txt".format(
                            simulation_round, tx_count, subsets, exp_block_count, zipf)
                        c_path = root_path / "cat_{}_round_{}_TXs_{}_subsets_{}_exp_blocks_{}_zipf.txt".format(
                            simulation_round, tx_count, subsets, exp_block_count, zipf)

                    s_f, c_f = open(s_path, "a+"), open(c_path, "a+")
                    np.set_printoptions(suppress=True)
                    print("f rate: {}".format(rate), file=s_f)
                    print("Latency (s): {}, Effective Throughput (tps): {}, Average Storage (byte): {}".
                          format(s_results[0], s_results[1], s_results[2]), file=s_f)

                    print("f rate: {}".format(rate), file=c_f)
                    print("Latency (s): {}, Effective Throughput (tps): {}, Average Storage (byte): {}".
                          format(c_results[0], c_results[1], c_results[2]), file=c_f)
                    s_f.close()
                    c_f.close()

                    end = time.time()
                    print("Got the result of one file! Time usage: {}".format(end-start))

        print('Done!')
